import re, unicodedata, secrets
from typing import List, Optional
from .config import SYSTEM_PROMPT
# ===== Cleaners =====
_HANJA_RE_ALL = re.compile(r"[\u3400-\u9FFF]")
_MD_FENCE_RE = re.compile(r"```.*?```", re.S)
_MD_INLINE_RE = re.compile(r"`[^`]+`")
_MD_LIST_RE = re.compile(r"^\s*(?:[\-\*\â€¢]|[0-9]+\.)\s+", re.M)
_MD_HDR_RE = re.compile(r"^\s*#{1,6}\s*", re.M)
_ROLE_MARKER = re.compile(r"\[(?:USER|ASSISTANT|ASSIGNANT|ASSIGNIGINANT|SYSTEM|ê°€ì´ë“œ)\]\s*", re.I)

def strip_markdown_noise(s: str) -> str:
    if not s: return s
    s = _MD_FENCE_RE.sub(" ", s)
    s = _MD_INLINE_RE.sub(" ", s)
    s = _MD_LIST_RE.sub("", s)
    s = _MD_HDR_RE.sub("", s)
    s = _ROLE_MARKER.sub("", s)
    s = re.sub(r"\[(?:[^\]]+)\]\([^)]+\)", " ", s)
    return s

_META_NOISE_RE = re.compile(r"(í•œê¸€ë§Œ\s*\(|\bë¬¸ì¥\s*[:ï¼š]|\bê²°ê³¼\s*[:ï¼š]|\bìš”ì•½\s*[:ï¼š]|\bì¶œë ¥\s*[:ï¼š]|```)", re.I)

def drop_meta_chunks(s: str) -> str:
    if not s: return s
    s = re.sub(r"í•œê¸€ë§Œ\s*\([^)]*\)", " ", s)
    s = re.sub(r"(ë¬¸ì¥|ê²°ê³¼|ìš”ì•½|ì¶œë ¥)\s*[:ï¼š]\s*", " ", s)
    sents = re.split(r"[.!?â€¦\n]+", s)
    kept = [t.strip() for t in sents if t.strip() and not _META_NOISE_RE.search(t)]
    return " ".join(kept).strip()

def split_sentences_ko(s: str) -> List[str]:
    parts = re.split(r"[.!?â€¦\n]+", s)
    return [p.strip() for p in parts if p and p.strip()]

def sanitize_korean_strict(s: str, *, max_sent: int = 3, fallback: Optional[str] = None) -> str:
    if not s: return (fallback or "").strip()
    s = unicodedata.normalize("NFC", str(s))
    s = strip_markdown_noise(s)
    s = drop_meta_chunks(s)
    s = _HANJA_RE_ALL.sub("", s)
    sents = split_sentences_ko(s)
    kept = []
    for t in sents:
        t = re.sub(r"\s+", " ", t).strip()
        if not t: continue
        kept.append(t)
    if not kept and fallback: kept = [fallback]
    kept = kept[:max_sent]
    out = " ".join(kept).strip()
    out = re.sub(r"(\d+)\s+(ë¶„|ì´ˆ|íšŒ|ê°œ|ì¥|ì¼|ì£¼|ì›”|ë…„|ì‹œê°„)", r"\1\2", out)
    out = re.sub(r"([ê°€-í£]+)\s+(ì€|ëŠ”|ì´|ê°€|ì„|ë¥¼|ê³¼|ì™€|ë¡œ|ìœ¼ë¡œ|ì—|ì—ì„œ|ì˜)", r"\1\2", out)
    out = re.sub(r"\s{2,}", " ", out).strip()
    return out

def looks_non_displayable(s: str) -> bool:
    if not s: return True
    core = re.findall(r"[ê°€-í£A-Za-z0-9]", s)
    if len(core) == 0: return True
    if s.count("?") >= max(3, len(s)//2): return True
    return False

def ko_text_fix(s: str) -> str:
    if not s: return s
    s = unicodedata.normalize("NFC", str(s))
    s = re.sub(r"\s{2,}", " ", s).strip()
    return s

# ===== Action bank & finalizer =====
ACTION_BANK = {
    "sleep": [
        "ì·¨ì¹¨1ì‹œê°„ ì „ í™”ë©´ì„ ë„ê³  ì¡°ëª…ì„ ë‚®ì¶°ë´ìš”.",
        "ì•ŒëŒì„ ê°™ì€ ì‹œê°„ìœ¼ë¡œ ë§ì¶”ê³  ì˜¤ëŠ˜ì€ 23ì‹œì— ë¶ˆì„ êº¼ë´ìš”.",
        "ì¹´í˜ì¸ì€ ì˜¤í›„2ì‹œ ì „ê¹Œì§€ë§Œ ë§ˆì…”ë´ìš”.",
        "ëˆ•ê¸° ì „ ë¯¸ì§€ê·¼í•œ ë¬¼ë¡œ 3ë¶„ ì†ë°œì„ ì”»ì–´ë³´ì„¸ìš”.",
    ],
    "interview": [
        "ì˜ˆìƒ ì§ˆë¬¸ 3ê°œë§Œ ì ê³  10ë¶„ê°„ í° ì†Œë¦¬ë¡œ ë¦¬í—ˆì„¤í•´ë´ìš”.",
        "STAR êµ¬ì¡°ë¡œ ì‚¬ë¡€ 1ê°œë§Œ ì •ë¦¬í•´ìš”.",
        "ê±°ìš¸ ì• ë¯¸ì†Œ 1ë¶„, ì²« ë¬¸ì¥ 5ë²ˆ ë§í•´ë³´ê¸°.",
    ],
    "food": [
        "ë¬¼ í•œ ì»µ ë§ˆì‹œê³  ìš”ê±°íŠ¸/ê³¼ì¼ì²˜ëŸ¼ ê°€ë²¼ìš´ ê°„ì‹ë¶€í„° ì‹œì‘í•´ìš”.",
        "ë°°ê³ í””ì„ 0~10ìœ¼ë¡œ ì²´í¬í•˜ê³  6 ì´ìƒì´ë©´ ì²œì²œíˆ í•œ ìˆŸê°ˆì”© ë“œì„¸ìš”.",
        "ë‹¨ ê²Œ ë‹¹ê¸°ë©´ ë‹¨ë°±ì§ˆ ê°„ì‹(ê³„ë€/ë‘ìœ ) ë¨¼ì € ë¨¹ì–´ë´ìš”.",
    ],
    "help_request": [
        "ì§€ê¸ˆ 3ë¶„ íƒ€ì´ë¨¸ë¥¼ ì¼œê³  ë– ì˜¤ë¥´ëŠ” ìƒê°ì„ ë©”ëª¨í•´ìš”.",
        "ê°€ì¥ ì‰¬ìš´ ì¼ 1ê°œë¥¼ 5ë¶„ë§Œ í•´ë´…ì‹œë‹¤.",
        "ì°½ë¬¸ì„ ì—´ê³  30ì´ˆ ê¹Šê²Œ ë“¤ìˆ¨Â·ë‚ ìˆ¨ 5íšŒ.",
    ],
    "smalltalk": [
        "ê·¸ëŸ° í•´í”„ë‹ë„ í•˜ë£¨ì— ì›ƒìŒì„ ì£¼ë„¤ìš”. 1ë¶„ ì–´ê¹¨ ëŒë¦¬ê³  ì´ì–´ê°€ìš” ğŸ™‚",
        "ì§€ê¸ˆ ëŠë‚Œì„ ì‚¬ì§„ í•œ ì¥ìœ¼ë¡œ ê¸°ë¡í•´ë³¼ê¹Œìš”?",
        "ì§§ê²Œ 1ë¶„ ìŠ¤íŠ¸ë ˆì¹­í•˜ê³  ê³„ì† ì´ì•¼ê¸°í•´ìš”.",
    ],
    "anger": [
        "ë§í•˜ê³  ì‹¶ì€ ë¬¸ì¥ì„ ì¢…ì´ì— ì“°ê³  10ë¶„ ë³´ë¥˜í•´ë´ìš”.",
        "4-4-6 í˜¸í¡ 5ë²ˆ: 4ì´ˆ ë“¤ìˆ¨, 4ì´ˆ ë©ˆì¶¤, 6ì´ˆ ë‚ ìˆ¨.",
        "â€˜ì§€ê¸ˆ í•  ê²ƒ/ë‚˜ì¤‘ì— í•  ê²ƒâ€™ìœ¼ë¡œ ì¢…ì´ë¥¼ ë°˜ì”© ë‚˜ëˆ  ì ì–´ë³´ê¸°.",
    ],
    "work": [
        "5ë¶„ì´ë©´ ëë‚  â€˜ì œì¼ ì‰¬ìš´ ì¼â€™ë¶€í„° ì‹œì‘í•´ìš”. ëë‚˜ë©´ ì²´í¬!",
        "ë°›ì€ í¸ì§€í•¨ 3ê°œë§Œ ì•„ì¹´ì´ë¸Œ/ì‚­ì œí•´ ë¨¸ë¦¬ë¥¼ ê°€ë³ê²Œ í•´ìš”.",
        "ì˜¤ëŠ˜ ëë‚¼ ê²ƒ 1ê°œë¥¼ ì¹´ë“œë¡œ í¬ê²Œ ì¨ì„œ ëˆˆì•ì— ë‘ì„¸ìš”.",
    ],
    "default": [
        "3ë¶„ íƒ€ì´ë¨¸ ì¼œê³  ìƒê°ì„ ê°€ë³ê²Œ ì ì–´ë´ìš”.",
        "ì°½ë¬¸ ì—´ê³  30ì´ˆ í˜¸í¡ í›„ ë¬¼ í•œ ì»µ ë§ˆì‹œê¸°.",
        "ê°€ì¥ ì‰¬ìš´ ì¼ 1ê°œë¥¼ 5ë¶„ë§Œ ì‹œë„í•´ë´ìš”.",
    ],
}

def pick_actions(intent: str, k: int = 1) -> list:
    pool = ACTION_BANK.get(intent) or ACTION_BANK.get("default", [])
    if not pool:
        return ["ì§€ê¸ˆ 3ë¶„ë§Œ í˜¸í¡ì„ ê°€ë‹¤ë“¬ê³ , ì‰¬ìš´ ì¼ í•œ ê°€ì§€ë¶€í„° ì‹œì‘í•´ë´ìš”."]
    arr = pool[:]; out = []
    import random
    for _ in range(min(k, len(arr))):
        choice = random.choice(arr); out.append(choice); arr.remove(choice)
    return out

def is_actionable(s: str) -> bool:
    return bool(re.search(r"(íƒ€ì´ë¨¸|ì§€ê¸ˆ|ì˜¤ëŠ˜|\d+\s*ë¶„|\d+\s*ì´ˆ|\d+\s*íšŒ|í•´ë³´|ì‹œë„í•´|ì¼œë³´|ë„)", s))

def _strip_greeting_and_identity(s: str) -> str:
    s = re.sub(r"^(ì•ˆë…•í•˜ì„¸ìš”|ì•ˆë…•|í•˜ì´)[^.\n]*[.\n]\s*", "", s.strip(), flags=re.I)
    s = re.sub(r"^(ì €ëŠ”|ë‚˜ëŠ”|AI|ì¸ê³µì§€ëŠ¥|ìƒë‹´ì‚¬|ë„ìš°ë¯¸)[^.\n]*[.\n]\s*", "", s.strip(), flags=re.I)
    s = re.sub(r"(ì €ëŠ”|ì €í¬|ì´ ëª¨ë¸ì€|ë³¸ ì‹œìŠ¤í…œì€)[^.\n]*ì…ë‹ˆë‹¤[.\n]\s*", "", s)
    return s.strip()

def finalize_reply(user_text: str, reply: str, *, intent: str = "help_request",
                   fallback: str = "ì§€ê¸ˆ 3ë¶„ë§Œ í˜¸í¡ì„ ê°€ë‹¤ë“¬ê³ , ê°€ì¥ ì‰¬ìš´ í•œ ê°€ì§€ë¥¼ 5ë¶„ë§Œ ì‹œì‘í•´ë´ìš”.") -> str:
    txt = sanitize_korean_strict(reply, max_sent=3, fallback=fallback)
    txt = strip_markdown_noise(txt).strip()
    txt = _strip_greeting_and_identity(txt)
    sents = split_sentences_ko(txt)
    if len(sents) > 3: txt = " ".join(sents[:3]).strip()
    if not is_actionable(txt):
        extra = pick_actions(intent, k=1)[0]
        txt = (txt + " " + extra).strip()
    return ko_text_fix(txt)

def fix_tags_list(tags: List[str]) -> List[str]:
    mapping = {"ë¶ˆë‚œ": "ë¶ˆì•ˆ", "ê±±ì¥": "ê±±ì •", "ë©´ì ‘ê¸°": "ë©´ì ‘", "ìˆ˜ë©´ì €í•˜": "ìˆ˜ë©´"}
    pool = []
    for t in (tags or []):
        t = str(t)
        parts = re.split(r"[^\wê°€-í£]+", t.replace("ï¼Œ", ",").replace("ã€", ",").replace(".", ","))
        for p in parts:
            p = (mapping.get(p.strip(), p.strip()))
            p = re.sub(r"[^ê°€-í£0-9]", "", p)[:12]
            if p and p not in pool: pool.append(p)
    return pool[:5]